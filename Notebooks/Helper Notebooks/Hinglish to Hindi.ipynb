{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a4fef7de-5634-4d03-b6e0-33b7d0435641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "61fddf14-6078-4426-bef3-ab35584c5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/char_hinglish_to_hindi.json') as file:\n",
    "    ch_mapping_str = file.read()\n",
    "    ch_mapping = json.loads(ch_mapping_str)\n",
    "\n",
    "with open('../datasets/word_hinglish_to_hindi.json') as file:\n",
    "    word_mapping_str = file.read()\n",
    "    word_mapping = json.loads(word_mapping_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "aeebbb65-04c3-4f35-942c-85f8c45065d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../datasets/char_hinglish_to_hindi.json', 'w') as file:\n",
    "#     ch_mapping_json = json.dumps(ch_mapping)\n",
    "#     file.write(ch_mapping_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "24af81ef-10d5-4afe-8eb3-9bf4b098cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/word_hinglish_to_hindi.json', 'w') as file:\n",
    "    word_mapping_json = json.dumps(word_mapping)\n",
    "    file.write(word_mapping_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c9f41b8d-2419-41dc-9544-baa709efd6a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, val in ch_mapping.items():\n",
    "    if type(val) == str:\n",
    "        ch_mapping[key] = [val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d063e585-084a-4619-96e3-b5225322e073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 27\n"
     ]
    }
   ],
   "source": [
    "# All english alphabets are covered as length 1 ch_mapping\n",
    "len_1_keys = [key for key in list(ch_mapping.keys()) if len(key) == 1]\n",
    "print(sorted(len_1_keys), len(len_1_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "62c56a56-c31c-40f4-99d0-62b2b1185dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mappings with length 4 should all have 'h' or 'r' at second index\n",
    "[key for key in list(ch_mapping.keys()) if len(key) == 4 and key[1] != 'h' and key[1] != 'r' and key[3] != 'n']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7763ca-570d-4c92-9219-3830720c9bea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a139406-c11a-4c1e-9e1d-ede17e800eef",
   "metadata": {},
   "source": [
    "## List of hindi vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3d42cfc2-1333-4d00-b603-124313e3d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/hindi_vocab.json', 'r') as file:\n",
    "    word_str = file.read()\n",
    "    hindi_vocab = json.loads(word_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a63ec57c-44e9-49f2-940f-87a941e4dc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116639, ['सन्धिवर्साय', 'प्रसङ्ग', 'विध्यान', 'नवफ', 'अहराई'])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi_vocab), hindi_vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4097351e-71f6-4340-84dc-7f3390927ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = ['a', 'b', 'c']\n",
    "lst[:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742cd089-74b9-4c50-bf68-53cf61f2d5ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f011e0e-5894-4a81-8d27-06c700c07d81",
   "metadata": {},
   "source": [
    "## Transliteration function for word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "16efc6ee-94a7-4f92-a905-a685e4931fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_word(\n",
    "    word: str = None, \n",
    "    dry_run: bool = False\n",
    ") -> list:    # x = input()\n",
    "    if dry_run:\n",
    "        print('\\n\\n-----------------------------')\n",
    "        print('Word: ', word)\n",
    "    # If word-level transliteration possible\n",
    "    if word in word_mapping.keys():\n",
    "        if dry_run: print('Word found in hinglish to hindi word mapping')\n",
    "        return [word_mapping[word]]\n",
    "    # Otherwise perform character-level transliteration\n",
    "    if len(word) == 0:\n",
    "        if dry_run: print('Word length is 0')\n",
    "        return []\n",
    "    if len(word) == 1 and word in ch_mapping:\n",
    "        if dry_run: print('Word length is 1')\n",
    "        return [mapped_w for mapped_w in ch_mapping[word] if '्' not in mapped_w]\n",
    "    if len(word) == 2 and len(word.strip()) == 2:\n",
    "        if dry_run: print('Word length is 2')\n",
    "        if (word.endswith('a') or word.endswith('i') or word.endswith('u')) and len(ch_mapping[word]) <= 2:\n",
    "            if dry_run: print('Word ends with a vowel so handling directly')\n",
    "            return [ch_mapping[word][-1]]\n",
    "        if (word.endswith('o') or word.endswith('e')) and len(ch_mapping[word]) <= 2:\n",
    "            if dry_run: print('Word ends with a vowel so handling directly')\n",
    "            return [ch_mapping[word][0]]\n",
    "\n",
    "    current_mappings = []\n",
    "    window_length = 3\n",
    "    if len(word) >=2:\n",
    "        second_letter = word[1]\n",
    "        if second_letter in ['r', 'h']:\n",
    "            window_length = 4\n",
    "    if len(word) >= 4 and word[3] == 'n':\n",
    "        window_length = 4\n",
    "    # In case you almost reached the end of string\n",
    "    window_length = min(window_length, len(word))\n",
    "    while window_length > 0:\n",
    "        if dry_run: print('Window Length: ', window_length)\n",
    "        window_word = word[0: window_length]\n",
    "        if dry_run: print('Window Word: ', window_word)\n",
    "        if window_word in word_mapping.values():\n",
    "            if dry_run: print('Window Word is a valid hindi word')\n",
    "            trans_words_for_window = [window_word]\n",
    "        elif len(window_word) == 1 and re.search(r'[\\u0900-\\u097F\\u200c\\u200d]', window_word):\n",
    "            if dry_run: print('Window Word is a valid hindi character')\n",
    "            trans_words_for_window = [window_word]    \n",
    "        elif window_word in ch_mapping:\n",
    "            if dry_run: print('Window Word found in hinglish to hindi character mapping')\n",
    "            trans_words_for_window = ch_mapping[window_word]\n",
    "        else:\n",
    "            if dry_run: print('Window Word is invalid')\n",
    "            window_length = window_length - 1\n",
    "            continue\n",
    "        if dry_run: print('Trans Words for Window Word: ', trans_words_for_window)\n",
    "        remaining_word = word[window_length: ]\n",
    "        if dry_run: print('Remaining Word: ', remaining_word)\n",
    "        if dry_run: print('Finding the trans words for remaining word ...')\n",
    "        if len(remaining_word) == 0:\n",
    "            if (len(window_word) == 3 and window_word[-2:] == 'an') or (len(window_word) == 2 and window_word[-1] == 'n'):\n",
    "                window_length = window_length - 1\n",
    "                continue\n",
    "            else:\n",
    "                return trans_words_for_window\n",
    "        else:\n",
    "            trans_words_for_remaining = transliterate_word(remaining_word, dry_run)\n",
    "        if dry_run: print('Trans Words for Remaining Word: ', trans_words_for_remaining)\n",
    "        for trans_word_p1 in trans_words_for_window:\n",
    "            for trans_word_p2 in trans_words_for_remaining:\n",
    "                joined_word = trans_word_p1 + trans_word_p2\n",
    "                current_mappings.append(joined_word)\n",
    "                if dry_run: print('Current Mappings: ', current_mappings)\n",
    "        if (len(window_word) == 3 and window_word[-2:] == 'an') or (len(window_word) == 2 and window_word[-1] == 'n'):\n",
    "            window_length = window_length - 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return current_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b8f06916-4e7d-4916-9066-905462c58444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, ['गर्भ्वति', 'गर्भ्वती', 'गर्भ्वटी', 'गर्भ्वटि', 'गर्भ्वाति'])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_word = 'garbhvati'\n",
    "possible_mappings = transliterate_word(eng_word)\n",
    "len(possible_mappings), possible_mappings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5caaf1-e722-4423-bd61-58dac519b7b9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181b5d8-5e32-4a22-8d72-f6d7bf302f10",
   "metadata": {},
   "source": [
    "## Tokenize Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1f5cea7e-1ee0-477c-bce1-9ef6952f707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, lang='hinglish'):\n",
    "    \"\"\"\n",
    "        lang = 'hinglish' mean both hindi and english\n",
    "             = 'hin'/'hindi'/'devanagari'/'dev'/'devanagri' mean only hindi\n",
    "             = 'eng'/'english' mean only english\n",
    "    \"\"\"\n",
    "    tokens = re.split(r'\\s', sentence)\n",
    "    invalid_char_regex = r'[^a-zA-Z\\u0900-\\u097F\\u200c\\u200d]'\n",
    "    if lang in ['hin', 'hindi', 'devanagari', 'dev', 'devanagri']:\n",
    "        invalid_char_regex = r'[^\\u0900-\\u097F\\u200c\\u200d]'\n",
    "    elif lang in ['eng', 'english']:\n",
    "        invalid_char_regex = r'[^a-zA-Z]'\n",
    "    tokens = [re.sub(invalid_char_regex, '', token) for token in tokens if token != '']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ec1ea5fd-00cd-4430-b6b9-7b7b23e122e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaj', 'mujhe', 'bahut', 'khushi', 'ho', 'rahi', 'hai']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = 'a!aj mu#$jhe \\nbahut khushi ho rahi hai'\n",
    "tokenize_sentence(english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42aa80c-8fe3-47b6-ba1e-28d57d935f95",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2e6c4-96df-4e0c-a5cd-6ff4ab1d9431",
   "metadata": {},
   "source": [
    "## Examples of Sentence Tokenization followed by Word-Level Transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "077c6d92-2d78-48d6-ba37-324c657bca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['आज']\n",
      "['मुझे']\n",
      "['बहुत']\n",
      "['खुशी']\n",
      "['हो']\n",
      "['रही', 'राही']\n",
      "['है']\n"
     ]
    }
   ],
   "source": [
    "english = 'a!aj mu#$jhe \\nbahut khushi ho rahi hai'\n",
    "eng_tokens = tokenize_sentence(english)\n",
    "for token in eng_tokens:\n",
    "    hindi_tokens = transliterate_word(token)\n",
    "    valid_tokens = [token for token in hindi_tokens if token in hindi_vocab]\n",
    "    print(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2abe5be9-7864-4b00-87f4-dc3e8b3c5c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['रामजी']\n",
      "['का']\n",
      "['जन्म', 'जनम', 'जानम']\n",
      "['और']\n",
      "['सीता']\n",
      "['जी']\n",
      "['से']\n",
      "['मिलन', 'मिलान']\n"
     ]
    }
   ],
   "source": [
    "english = 'ramji ka janm aur sita ji se milan'\n",
    "eng_tokens = tokenize_sentence(english)\n",
    "for token in eng_tokens:\n",
    "    hindi_tokens = transliterate_word(token)\n",
    "    valid_tokens = [token for token in hindi_tokens if token in hindi_vocab]\n",
    "    if len(valid_tokens) == 0:\n",
    "        print(hindi_tokens)\n",
    "    else:\n",
    "        print(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9b473485-90c1-4e4b-9715-b4a20642f2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['प्रेम']\n",
      "['ही']\n",
      "['कृष्णा', 'कृष्ण', 'कृषण']\n"
     ]
    }
   ],
   "source": [
    "english = 'prem hi krishna'\n",
    "eng_tokens = tokenize_sentence(english)\n",
    "for token in eng_tokens:\n",
    "    hindi_tokens = transliterate_word(token)\n",
    "    valid_tokens = [token for token in hindi_tokens if token in hindi_vocab]\n",
    "    if len(valid_tokens) == 0:\n",
    "        print(hindi_tokens)\n",
    "    else:\n",
    "        print(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e02f6809-85f4-4d93-9be9-cf1c496adb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['तेरे']\n",
      "['भाग', 'भाज']\n",
      "['में', 'मैं']\n",
      "['कुछ']\n",
      "['और']\n",
      "['ही']\n",
      "['लिख', 'लिखा']\n",
      "['है']\n"
     ]
    }
   ],
   "source": [
    "english = 'tere bhaag mein kuch aur hi likha hai'\n",
    "eng_tokens = tokenize_sentence(english)\n",
    "for token in eng_tokens:\n",
    "    hindi_tokens = transliterate_word(token)\n",
    "    valid_tokens = [token for token in hindi_tokens if token in hindi_vocab]\n",
    "    if len(valid_tokens) == 0:\n",
    "        print(hindi_tokens)\n",
    "    else:\n",
    "        print(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1b131b65-da85-4abd-be7a-3e262e681c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मैं']\n",
      "['क्या']\n",
      "['बताऊ']\n"
     ]
    }
   ],
   "source": [
    "english = 'mai kya batau'\n",
    "eng_tokens = tokenize_sentence(english)\n",
    "for token in eng_tokens:\n",
    "    hindi_tokens = transliterate_word(token)\n",
    "    valid_tokens = [token for token in hindi_tokens if token in hindi_vocab]\n",
    "    if len(valid_tokens) == 0:\n",
    "        print(hindi_tokens)\n",
    "    else:\n",
    "        print(valid_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4961044-8f7e-418a-926e-8fdfdf335798",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0966e646-5fe8-4cdd-a82d-9eedd30633e0",
   "metadata": {},
   "source": [
    "## Sentence-Level Transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cecd7fee-7ec3-473a-b04c-1cd29bf18866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_sentence(\n",
    "    sentence: str = None, \n",
    "    dry_run: bool = False\n",
    ") -> list:\n",
    "    eng_tokens = tokenize_sentence(sentence)\n",
    "    for i, token in enumerate(eng_tokens):\n",
    "        if token in word_mapping:\n",
    "            eng_tokens[i] = word_mapping[token]\n",
    "        if len(token) == 2:\n",
    "            if (token.endswith('a') or token.endswith('i') or token.endswith('u')) and len(ch_mapping[token]) <= 2:\n",
    "                eng_tokens[i] = ch_mapping[token][-1]\n",
    "            if (token.endswith('o') or token.endswith('e')) and len(ch_mapping[token]) <= 2:\n",
    "                eng_tokens[i] = ch_mapping[token][0]\n",
    "    mid_sent = ' '.join(eng_tokens)\n",
    "    if dry_run: print('Mapped sentence: ', mid_sent)\n",
    "    hindi_sent_list = [mid_sent]\n",
    "    if re.search(r'[a-zA-Z]', mid_sent):\n",
    "        hindi_sent_list = transliterate_word(mid_sent, dry_run=dry_run)\n",
    "    valid_sent_list = []\n",
    "    if dry_run: print('\\n\\n')\n",
    "    for i, hindi_sent in enumerate(hindi_sent_list):\n",
    "        if dry_run: print(f'\\n\\nHindi sentence possibility {i+1}: ', hindi_sent)\n",
    "        hin_tokens = tokenize_sentence(hindi_sent, 'hin')\n",
    "        invalid_sent = False\n",
    "        for token in hin_tokens:\n",
    "            if token not in hindi_vocab or token.endswith('्'):\n",
    "                if dry_run: print('This sentence possibility is invalid due to the token ', token)\n",
    "                invalid_sent = True\n",
    "                break\n",
    "        if not invalid_sent:\n",
    "            if dry_run: print('This sentence possibility is valid')\n",
    "            valid_sent_list.append(hindi_sent)\n",
    "    if len(valid_sent_list) == 0:\n",
    "        return hindi_sent_list\n",
    "    else:\n",
    "        return valid_sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "237de04f-4ec6-4bba-97a0-8be5ff8eab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "राम और सीता का विवाह\n"
     ]
    }
   ],
   "source": [
    "english_sent = 'ram aur sita ka vivah'\n",
    "possible_sents = transliterate_sentence(english_sent, dry_run=False)\n",
    "for sent in possible_sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad1b4b-50fb-4e42-9674-228cd53ac1ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc781a-4d10-4312-8b2e-942ef6fa4f3e",
   "metadata": {},
   "source": [
    "## Combining word and sentence tokenization using product (useful for caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "61cd3153-60eb-49b1-a450-ffb46a81d3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "मैं क्या बताऊ\n"
     ]
    }
   ],
   "source": [
    "english = 'mai kya batau'\n",
    "eng_tokens = tokenize_sentence(english)\n",
    "tokens_mapped = []\n",
    "for token in eng_tokens:\n",
    "    hindi_tokens = transliterate_sentence(token)\n",
    "    tokens_mapped.append(hindi_tokens)\n",
    "possible_sents = [' '.join(words) for words in product(*tokens_mapped)]\n",
    "for sentence in possible_sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9b885be1-0fbd-461f-9331-43ef5fd5534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "कठिनाई\n"
     ]
    }
   ],
   "source": [
    "english = 'kathinaai'\n",
    "eng_tokens = tokenize_sentence(english)\n",
    "tokens_mapped = []\n",
    "for token in eng_tokens:\n",
    "    hindi_tokens = transliterate_sentence(token)\n",
    "    tokens_mapped.append(hindi_tokens)\n",
    "possible_sents = [' '.join(words) for words in product(*tokens_mapped)]\n",
    "for sentence in possible_sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03cc1c5-f7a3-4a22-a4f7-edea90594f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (hta-env)",
   "language": "python",
   "name": "hta-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
